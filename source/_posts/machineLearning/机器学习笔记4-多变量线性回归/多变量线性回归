机器学习笔记4 多元梯度下降法

1.多特征
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%A4%9A%E5%85%83%E7%89%B9%E5%BE%81.png)

2.多元特征下降法
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png)

3.多元特征下降法-特征缩放
有多个变量来求全局最优解的时候，如果变量的取值范围非常不一样，会使得等高线图变得扁平，比如图中的房屋尺寸和房间数量，
一个是0-2000另外一个是1-5，会导致求全局最优解变得很慢，
要花很长时间来计算。所以这里要把特征的范围缩小到比较相近的范围，比如x1/2000，x2/5,这样x1和x2都的范围是[0,1],使等高线的图看起来比较圆，
会更快的找到全局最优解。
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.png)
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E5%8F%96%E5%80%BC%E8%AF%B4%E6%98%8E.png)

均值归一化 mean normalization:
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96.png)
μ1是x1的平均值，s1是x1的取值范围，一般就是x1的最大值减去x1的最小值。

上面就是使用特征缩放的介绍，使用这个方法可以大大减少收敛的数，提高计算的效率。
4.多元特征下降法-学习率
梯度下降中的学习率α，该怎么选择是这一节要讨论的问题。
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%AD%A6%E4%B9%A0%E7%8E%87.png)


当上一次的迭代出来的结果和当前迭代的结果差值不超过10的-3次幂，基本认为是已经收敛了，没有必要继续迭代了。但是实际情况中，有的算法
30次迭代就收敛了，有的可能需要三百万次才能迭代收敛，取决于不同的场景和算法。这个迭代次数和什么时候停止是很难确定的。

![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%80%BB%E7%BB%93.png)
总结：
如果学习率太小，会导致收敛得很慢
如果学习率太大，代价函数可能不是每次都会下降，可能不收敛
所以为了找到比较正确的学习率，最好画出代价函数的图形，根据图形来判断学习率的选择。

每次3倍的增加学习率，会找到最大和最小的学习率，最后找到的值可能是比最大的学习率稍微小一点的值。

5.特征和多项式回归
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92.png)
像上图中的例子，如果对线性方程很熟悉就会想到用平方根的函数来拟合。所以多项式的回归没有固定的方法，熟悉之后可以用各种方式来拟合出方程。

6.正规方程（区别于迭代方法的直接解法）
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png)
正规方程：x的转置乘以x的结果的逆，乘以x的转置，乘以y就可以得到θ的值。
(没有讲述怎么得来的这个方程，计算就完事了)
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E5%AF%BC%E7%AE%97%E6%9C%80%E5%B0%8F%E5%80%BC.png)
http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8D%E9%9C%80%E8%A6%81%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.png
正规方程不需要使用特征缩放，可以直接通过计算得出结果。

![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%BC%98%E7%BC%BA%E7%82%B9%E5%AF%B9%E6%AF%94.png)
正规方程和梯度下降的优缺点对比：
正规方程优点在于不用选择学习率，不需要迭代计算。缺点在于无法进行n太大的情况的计算，因为正规方程时间复杂度是O(n^3)，所以复杂度太高了，计算耗时很长，
一般当n的值大于一万就不要用正规方程了。
梯度下降的优点在于可以计算很大的n的情况，而且工作得也特别好，但是确定是要选择一个学习率阿尔法，而且需要不停的迭代计算。



7.正规方程（在矩阵不可逆的情况下的解法）
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E4%B8%8D%E5%8F%AF%E9%80%86.png)
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%B8%8D%E5%8F%AF%E9%80%86%E7%9A%84%E8%A7%A3%E6%B3%95.png)
矩阵一般不会出现不可逆的情况，如果真的出现了可以使用以下的方式来解决：
1.检查参数是不是有固定关系的特征，比如x1是平方米，x2是平方英尺，那么这两个的函数只有固定的换算关系的，可能会导致正规方程不可逆，这时候要检查特征去掉重复的
2.如果有非常多个特征，导致矩阵不可逆，要删除一些特征在计算
