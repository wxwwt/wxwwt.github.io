### hadoop启动dfs异常 util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

记录一下安装hadoop遇到的问题，以便后面再次遇到有个解决方式的记录，也方便其他人解决相同的问题。在安装好hadoop后，start-dfs.sh的时候报了util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable。
从字面上来看就是本地的hadoop的库并不适用于本机的平台。网上搜索后发现可能是以下三种问题导致的：



问题一：native-hadoop的库是32位的，而系统是64位的导致不可用。
进入到hadoop目录下的lib/native目录下，linux系统使用
```
file libhadoop.so.1.0.0
得到的结果是
libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, BuildID[sha1]=2c6a0dae993e827ec637437f921b30279487049c, not stripped
```
可以看到本地hadoop的库是64位的，我的电脑也是64位的。所以应该不是这个版本的问题

问题二：/etc/profile中没有添加依赖库的地址。这个确实我没有添加过
(1).需要在/etc/profile中添加
```
export HADOOP_HOME=/Users/scott/hadoop-3.2.0
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"
```
然后在使用source /etc/profile重新执行该文件使配置生效。
(2).需要在hadoop-env.sh添加同样的配置

注：我这一步弄完其实hdfs就可以启动起来了，但是依然会报标题那个warn。但是不影响使用了。



问题三：libhadoop.so.1.0.0缺少了依赖库或者
#### linux：
使用ldd命令去查看libhadoop.so.1.0.0文件是不是缺少了动态依赖库
如果出现
```
./libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./libhadoop.so.1.0.0)
        linux-vdso.so.1 =>  (0x00007fff369ff000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007f3caa7ea000)
        libc.so.6 => /lib64/libc.so.6 (0x00007f3caa455000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f3caac1b000)
```
这种类似的就是glibc版本问题，使用ldd --version可以查看版本。
```
# ldd --version
ldd (GNU libc) 2.13
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
Written by Roland McGrath and Ulrich Drepper.

```
如果发现版本是低于刚才上面出现的GLIBC_2.14,就需要升级glibc版本。这个比较麻烦，而且有一定风险，我遇到的不是这个问题，所以没有实践这个操作。可以参考[升级glibc版本](https://blog.csdn.net/l1028386804/article/details/88420473)。

#### mac：
还有一种情况呢，就是有的朋友可能是在mac上安装的hadoop，我也这样尝试了。首先排除了问题一后，问题二基本上就解决了问题。如果尝试问题三会发现，mac上没有ldd这个命令，网上说使用otool -L 文件，这样是行不通的。
libhadoop.so是C写的。需要安装binutils，然后使用binutils中的readelf命令，readelf -d 文件|grep NEED
![](http://wxwwt-oss.oss-cn-hangzhou.aliyuncs.com/article_picture/hadoophdfs%E5%90%AF%E5%8A%A8%E5%BC%82%E5%B8%B8/readelf.jpg)
这里-d就是查看动态依赖库，grep NEED就是需要的库。
不过这里其实只是列举一下readelf的用法

如果要去除这个warn可以在hadoop的文件夹下的/etc/hadoop/log4j.properties中添加
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
把错误级别调成ERROR就不会报这个warn了。
